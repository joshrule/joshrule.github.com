@inproceedings{sammons2009relation,
  title = {Relation alignment for textual entailment recognition},
  author = {Sammons, Mark and Vydiswaran, V G Vinod and Vieira, Tim and Johri, Nikhil and Chang, Ming-Wei and Goldwasser, Dan and Srikumar, Vivek and Kundu, Gourab and Tu, Yuancheng and Small, Kevin and Rule, Joshua and Do, Quang and Roth, Dan},
  date = {2009},
  abstract = {We present an approach to textual entailment recognition, in which inference is based on a shallow semantic representation of relations (predicates and their arguments) in the text and hypothesis of the entailment pair, and in which specialized knowledge is encapsulated in modular components with very simple interfaces. We propose an architecture designed to integrate different, unscaled Natural Language Processing resources, and demonstrate an alignment-based method for combining them. We clarify the purpose of alignment in the RTE task, identifying two distinct alignment models, each of which leads to a different type of entailment system. We identify desirable properties of alignment, and use this to inform our implementation of an alignment component. We evaluate the resulting system on the RTE5 data set, and use an ablation study to assess the conformance of our alignment approach with these desired characteristics.},
  langid = {english},
  keywords = {proceedings},
  booktitle = {Proceedings of the Textual Alignment Conference},
  file = {/Users/rule/sync/josh/references/papers/sammons2009relation.pdf},
  index = {1}
}
@inproceedings{dechter2014unsupervised,
  title = {Unsupervised Learning of Probabilistic Programs with Latent Predicate Networks},
  author = {Dechter, Eyal and Rule, Joshua and Tenenbaum, Joshua B.},
  date = {2014},
  booktitle = {Proceedings of the NeurIPS 2014 Workshop on Probabilistic Programming},
  keywords = {misc},
  annotation = {Published: Poster and abstract},
  index = {2}
}
@inproceedings{dechter2015latent,
  title = {Latent {{Predicate Networks}}: Concept {{Learning}} with {{Probabilistic Context}}-{{Sensitive Grammars}}},
  author = {Dechter, Eyal and Rule, Joshua and Tenenbaum, Joshua B},
  date = {2015},
  abstract = {For humans, learning abstract concepts and learning language go hand in hand: we acquire abstract knowledge primarily through linguistic experience, and acquiring abstract concepts is a crucial step in learning the meanings of linguistic expressions. Number knowledge is a case in point: we largely acquire concepts such as seventy-three through linguistic means, and we can only know what the sentence ``seventy-three is more than twice as big as thirty-one'' means if we can grasp the meanings of its component number words. How do we begin to solve this problem? One approach is to estimate the distribution from which sentences are drawn, and, in doing so, infer the latent concepts and relationships that best explain those sentences. We present early work on a learning framework called Latent Predicate Networks (LPNs) which learns concepts by inferring the parameters of probabilistic context-sensitive grammars over sentences. We show that for a small fragment of sentences expressing relationships between English number words, we can use hierarchical Bayesian inference to learn grammars that can answer simple queries about previously unseen relationships within this domain. These generalizations demonstrate LPNs' promise as a tool for learning and representing conceptual knowledge in language.},
  booktitle = {Proceedings of the AAAI 2015 Spring Symposium},
  langid = {english},
  keywords = {proceedings},
  file = {/Users/rule/sync/josh/references/papers/dechter2015latent.pdf},
  index = {3}
}
@article{glezer2015adding,
  title = {Adding {{Words}} to the {{Brain}}'s {{Visual Dictionary}}: Novel {{Word Learning Selectively Sharpens Orthographic Representations}} in the {{VWFA}}},
  shorttitle = {Adding {{Words}} to the {{Brain}}'s {{Visual Dictionary}}},
  author = {Glezer, L. S. and Kim, J. and Rule, J. and Jiang, X. and Riesenhuber, M.},
  date = {2015},
  journaltitle = {Journal of Neuroscience},
  volume = {35},
  number = {12},
  langid = {english},
  keywords = {article},
  file = {/Users/rule/sync/josh/references/papers/glezer2015adding.pdf},
  index = {4}
}
@inproceedings{rule2015representing,
  title = {Representing and learning a large system of number concepts with {{Latent Predicate Networks}}},
  booktitle = {Proceedings of the Cognitive Science Society},
  author = {Rule, Joshua and Dechter, Eyal and Tenenbaum, Joshua B},
  date = {2015},
  keywords = {misc},
  index = {5}
}
@inproceedings{rule2018learning,
  title = {Learning list concepts through program induction},
  booktitle = {Proceedings of the Cognitive Science Society},
  author = {Rule, Joshua and Schulz, Eric and Piantadosi, Steven T. and Tenenbaum, Joshua B.},
  date = {2018},
  abstract = {Humans master complex systems of interrelated concepts like mathematics and natural language. Previous work suggests learning these systems relies on iteratively and directly revising a language-like conceptual representation. We introduce and assess a novel concept learning paradigm called Martha's Magical Machines that captures complex relationships between concepts. We model human concept learning in this paradigm as a search in the space of term rewriting systems, previously developed as an abstract model of computation. Our model accurately predicts that participants learn some transformations more easily than others and that they learn harder concepts more easily using a bootstrapping curriculum focused on their compositional parts. Our results suggest that term rewriting systems may be a useful model of human conceptual representations.},
  keywords = {proceedings},
  file = {/Users/rule/sync/josh/references/papers/rule2018learning.pdf},
  index = {6}
}
@inproceedings{rule2019learning,
  title = {Learning a novel rule-based conceptual system},
  author = {Rule, Joshua S and Piantadosi, Steven T and Tenenbaum, Joshua B},
  booktitle = {Proceedings of the Cognitive Science Society},
  date = {2019},
  abstract = {Humans have developed complex rule-based systems to explain and exploit the world around them. When a learner has already mastered a system's core dynamics\textemdash identifying its primitives and their interrelations\textemdash further learning can be effectively modeled as discovering useful compositions of these primitives. It nevertheless remains unclear how the dynamics themselves might initially be acquired. Composing primitives is no longer a viable strategy, as the primitives themselves are what must be explained. To explore this problem, we introduce and assess a novel concept learning paradigm in which participants use a two-alternative forced-choice task to learn an unfamiliar rule-based conceptual system: the MUI system (Hofstadter, 1980). We show that participants reliably learn this system given a few dozen examples of the system's rules, leaving open the mechanism by which novel conceptual systems are acquired but providing a useful paradigm for further study.},
  langid = {english},
  keywords = {proceedings},
  file = {/Users/rule/sync/josh/references/papers/rule2019learning.pdf},
  index = {7}
}
@thesis{rule2020child,
  type = {Doctoral Thesis},
  title = {The Child as Hacker: Building More Human-like Models of Learning},
  author = {Rule, Joshua S},
  date = {2020},
  institution = {MIT},
  abstract = {Cognitive science faces a radical challenge in explaining the richness of human learning and cognitive development. This thesis proposes that developmental theories can address the challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that learning from childhood onward is analogous to a style of programming called hacking\textemdash making code better along many dimensions through an open-ended and internally-motivated set of diverse values and activities. This thesis also develops a first attempt to formalize and assess the child as hacker view through an in-depth empirical study of human and machine concept learning. It introduces list functions as a domain for psychological investigation, demonstrating how they subsume many classic concept learning tasks while opening new avenues for exploring algorithmic thinking over complex structures. It also presents HL, a computational learning model whose representations, objectives, and mechanisms reflect core principles of hacking. Existing work on concept learning shows that learners both prefer simple explanations of data and find them easier to learn than complex ones. The child as hacker, by contrast, suggests that learners use mechanisms that dissociate hypothesis complexity and learning difficulty for certain problem classes. We thus conduct a large-scale experiment exploring list functions that vary widely in difficulty and algorithmic content to help identify structural sources of learning difficulty. We find that while description length alone predicts learning, predictions are much better when accounting for concepts' semantic features. These include the use of internal arguments, counting knowledge, case-based and recursive reasoning, and visibility\textemdash a measure we introduce to modify description length based on the complexity of inferring each symbol in a description. We further show that HL's hacker-like design uses these semantic features to better predict human performance than several alternative models of learning as programming. These results lay groundwork for a new generation of computational models and demonstrate how the child as hacker hypothesis can productively contribute to our understanding of learning.},
  langid = {english},
  keywords = {thesis},
  file = {/Users/rule/sync/josh/references/papers/rule2020child.pdf},
  index = {8}
}
@article{rule2020child2,
  title = {The child as hacker},
  author = {Rule, Joshua S. and Piantadosi, Steven T. and Tenenbaum, Joshua B.},
  date = {2020},
  journaltitle = {Trends in Cognitive Sciences},
  keywords = {article},
  file = {/Users/rule/sync/josh/references/papers/rule2020child2.pdf},
  index = {9}
}
@article{rule2021leveraging,
  title = {Leveraging prior concept learning in computational models of human object recognition},
  author = {Rule, Joshua S. and Riesenhuber, Maximilian},
  date = {2021},
  journaltitle={Frontiers in Computational Neuroscience},
  keywords = {article},
  index = {10}
}
